{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is created based on `mlxtend` library's documentation page ([link](https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/)). This notebook demonstrates some basic concepts of association analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a toy data set for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Garlic', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare (transform) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "\n",
    "te_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te.columns_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a dataframe from these results\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Support`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate `support` for each individual product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_supports = df.sum() / len(df)\n",
    "\n",
    "item_supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_supports = item_supports.sort_values(ascending=False)\n",
    "\n",
    "print (item_supports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_supports.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all two-way combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take the first row as an example first\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all two-way combinations\n",
    "list(combinations(dataset[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this approach to calculate `support` for each pair of items _from the entire dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each two-way item pair into an array\n",
    "item_pairs = []\n",
    "\n",
    "for i, order in enumerate(dataset):\n",
    "    pairs = combinations(set(order), 2)\n",
    "    \n",
    "    # for each product pair\n",
    "    for item_pair in pairs:\n",
    "        item_pairs.append(item_pair)\n",
    "        \n",
    "len(item_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how frequent each item-pair is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(tuple(sorted(elem)) for elem in item_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must use `sorted()` option here because otherwise (Eggs, Kidney Beans) is considered a different set than (Kidney Beans, Eggs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the results in a dictionary and sort it in descending order of frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_pair_ct = Counter(tuple(sorted(elem)) for elem in item_pairs)\n",
    "\n",
    "item_pair_ct.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the array ot tuples from high to low frequency\n",
    "sorted(item_pair_ct.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the sorted results\n",
    "item_pair_ct_sorted = sorted(item_pair_ct.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# let's calculate the percentages (i.e., support) from these counts\n",
    "item_pair_pct_sorted = {}\n",
    "\n",
    "for i, item_pair in enumerate(item_pair_ct_sorted):\n",
    "    item_pair_pct_sorted[item_pair[0]] = item_pair[1] / len(dataset)\n",
    "    \n",
    "print (item_pair_pct_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Support` Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this exercise, we will use a support threshold of 0.6\n",
    "min_support = 0.5\n",
    "\n",
    "# extract all items that satisfy the support criterion\n",
    "item_supports[item_supports >= min_support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = item_supports.plot.bar()\n",
    "ax.axhline(min_support, c='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all item-sets that satisfy the support criterion\n",
    "for key, value in item_pair_pct_sorted.items():\n",
    "    if value >= min_support:\n",
    "        print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing all these calculations manually, we can use `mlxtend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in rules according to a different metric of interest, you can simply adjust the metric and `min_threshold` arguments . E.g. if you are only interested in rules that have a `lift` of >= 1.2, you would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.2)\n",
    "\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames make it easy to filter the results further. Let's say we are ony interested in rules that satisfy the following criteria:\n",
    "\n",
    "1. at least 2 antecedents\n",
    "2. a confidence > 0.75\n",
    "3. a lift score > 1.2\n",
    "\n",
    "We could compute the antecedent length as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules['antecedent_len'] = rules['antecedents'].apply(lambda x: len(x))\n",
    "\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use pandas' selection syntax as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[ (rules['antecedent_len'] >= 2) &\n",
    "       (rules['confidence'] > 0.75) &\n",
    "       (rules['lift'] > 1.2) ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
